{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Setup + sample data</h3>",
   "id": "600dfde25fe5291a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T03:25:24.832576Z",
     "start_time": "2025-07-28T03:25:24.823013Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# ---- Sample data ----\n",
    "df = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3, 4, 5],\n",
    "    \"gender\": [\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"],\n",
    "    \"region\": [\"North\", \"South\", \"East\", \"West\", \"South\"],\n",
    "    \"product_category\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n",
    "    \"spend\": [120.5, 99.9, 233.0, 150.75, 80.0]  # numeric column left as-is\n",
    "})\n",
    "\n",
    "print(df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  gender region product_category   spend\n",
      "0   1    Male  North                A  120.50\n",
      "1   2  Female  South                B   99.90\n",
      "2   3  Female   East                A  233.00\n",
      "3   4    Male   West                C  150.75\n",
      "4   5    Male  South                B   80.00\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Identify categorical columns</h3>",
   "id": "de5d4ebab9caaff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T03:25:34.326818Z",
     "start_time": "2025-07-28T03:25:34.322497Z"
    }
   },
   "cell_type": "code",
   "source": "categorical_cols = [\"gender\", \"region\", \"product_category\"]\n",
   "id": "1040278d3c9034e5",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Fit & transform with OneHotEncoder</h3>\n",
    "Note: In scikit-learn ≥1.2, use sparse_output=False. In older versions, use sparse=False."
   ],
   "id": "f8d9a4c4575a9a7a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T03:25:45.821041Z",
     "start_time": "2025-07-28T03:25:45.805933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit & transform with OneHotEncoder\n",
    "# Note: In scikit-learn ≥1.2, use sparse_output=False. In older versions, use sparse=False.\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)  # or sparse=False if older sklearn\n",
    "encoded_array = enc.fit_transform(df[categorical_cols])\n",
    "\n",
    "# Get readable column names\n",
    "encoded_cols = enc.get_feature_names_out(categorical_cols)\n",
    "\n",
    "# Wrap back into a DataFrame\n",
    "df_encoded = pd.DataFrame(encoded_array, columns=encoded_cols, index=df.index)\n",
    "\n",
    "print(df_encoded.head())\n"
   ],
   "id": "13f8a27e63027001",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender_Female  gender_Male  region_East  region_North  region_South  \\\n",
      "0            0.0          1.0          0.0           1.0           0.0   \n",
      "1            1.0          0.0          0.0           0.0           1.0   \n",
      "2            1.0          0.0          1.0           0.0           0.0   \n",
      "3            0.0          1.0          0.0           0.0           0.0   \n",
      "4            0.0          1.0          0.0           0.0           1.0   \n",
      "\n",
      "   region_West  product_category_A  product_category_B  product_category_C  \n",
      "0          0.0                 1.0                 0.0                 0.0  \n",
      "1          0.0                 0.0                 1.0                 0.0  \n",
      "2          0.0                 1.0                 0.0                 0.0  \n",
      "3          1.0                 0.0                 0.0                 1.0  \n",
      "4          0.0                 0.0                 1.0                 0.0  \n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Merge back with the original DataFrame</h3>",
   "id": "11e203df893aa522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T03:26:20.906100Z",
     "start_time": "2025-07-28T03:26:20.891445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_final = pd.concat([df.drop(columns=categorical_cols), df_encoded], axis=1)\n",
    "print(df_final)\n"
   ],
   "id": "6fbd181007a13fcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   spend  gender_Female  gender_Male  region_East  region_North  \\\n",
      "0   1  120.50            0.0          1.0          0.0           1.0   \n",
      "1   2   99.90            1.0          0.0          0.0           0.0   \n",
      "2   3  233.00            1.0          0.0          1.0           0.0   \n",
      "3   4  150.75            0.0          1.0          0.0           0.0   \n",
      "4   5   80.00            0.0          1.0          0.0           0.0   \n",
      "\n",
      "   region_South  region_West  product_category_A  product_category_B  \\\n",
      "0           0.0          0.0                 1.0                 0.0   \n",
      "1           1.0          0.0                 0.0                 1.0   \n",
      "2           0.0          0.0                 1.0                 0.0   \n",
      "3           0.0          1.0                 0.0                 0.0   \n",
      "4           1.0          0.0                 0.0                 1.0   \n",
      "\n",
      "   product_category_C  \n",
      "0                 0.0  \n",
      "1                 0.0  \n",
      "2                 0.0  \n",
      "3                 1.0  \n",
      "4                 0.0  \n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>(Optional) Drop one level per category to avoid multicollinearity</h3>\n",
    "# If you plan to use linear models / logistic regression and want to avoid the dummy variable trap:"
   ],
   "id": "e36df44362bd5309"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T03:27:08.107207Z",
     "start_time": "2025-07-28T03:27:08.090423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enc_drop = OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse_output=False)\n",
    "encoded_drop = enc_drop.fit_transform(df[categorical_cols])\n",
    "encoded_cols_drop = enc_drop.get_feature_names_out(categorical_cols)\n",
    "df_encoded_drop = pd.DataFrame(encoded_drop, columns=encoded_cols_drop, index=df.index)\n",
    "\n",
    "df_final_drop = pd.concat([df.drop(columns=categorical_cols), df_encoded_drop], axis=1)\n",
    "print(df_final_drop)\n"
   ],
   "id": "4c9d9c3bbaedbf70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   spend  gender_Male  region_North  region_South  region_West  \\\n",
      "0   1  120.50          1.0           1.0           0.0          0.0   \n",
      "1   2   99.90          0.0           0.0           1.0          0.0   \n",
      "2   3  233.00          0.0           0.0           0.0          0.0   \n",
      "3   4  150.75          1.0           0.0           0.0          1.0   \n",
      "4   5   80.00          1.0           0.0           1.0          0.0   \n",
      "\n",
      "   product_category_B  product_category_C  \n",
      "0                 0.0                 0.0  \n",
      "1                 1.0                 0.0  \n",
      "2                 0.0                 0.0  \n",
      "3                 0.0                 1.0  \n",
      "4                 1.0                 0.0  \n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Doing it the “pipeline way” (recommended for ML)</h3>",
   "id": "8a513ca06697060e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T03:27:29.633893Z",
     "start_time": "2025-07-28T03:27:29.627972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "numeric_cols = [\"spend\"]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"  # or 'passthrough' to keep untouched columns\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# X = features; y = your target column (add one if you have it)\n",
    "#pipe.fit(X, y)\n"
   ],
   "id": "56d1a9b406317b2d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Key options you should know</h3>\n",
    "<li>handle_unknown=\"ignore\": prevents errors when unseen categories appear in test/inference data.</li>\n",
    "<li>drop=\"first\": drops the first level of each categorical variable to reduce collinearity.</li>\n",
    "<li>sparse_output=False: returns a dense NumPy array (easy to convert to DataFrame). Use the default sparse matrix for large, high-cardinality data to save memory.</li>"
   ],
   "id": "c914385448401cde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Context: One-Hot Encoding of a Categorical Column</h3>",
   "id": "bb26fe809266802b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Suppose we have a column Color with 3 unique values: Red, Blue, and Green\n",
    "If we apply one-hot encoding, we get 3 binary columns:\n",
    "| Color | Color_Red | Color_Blue | Color_Green |\n",
    "|-------|-----------|------------|-------------|\n",
    "| Red   | 1         | 0          | 0           |\n",
    "| Blue  | 0         | 1          | 0           |\n",
    "| Green | 0         | 0          | 1           |\n"
   ],
   "id": "39fb5b07c5ffaa11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now comes the key problem:\n",
    "If you know two of these columns, you can always figure out the third.\n",
    "\n",
    "Let’s say you have:\n",
    "Color_Red = 0\n",
    "Color_Blue = 0\n",
    "Then you must have:\n",
    "Color_Green = 1\n",
    "\n",
    "That’s perfectly predictable — meaning the third column is just a combination of the other two.\n",
    "\n"
   ],
   "id": "c12f1fd37390a35f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Why this causes issues:\n",
    "This is called perfect multicollinearity, where one column is a linear combination of others.\n",
    "\n",
    "In math terms:\n",
    "Color_Green = 1 - Color_Red - Color_Blue\n",
    "\n",
    "This is a problem for models like linear regression and logistic regression, which assume that all input variables are independent.\n",
    "\n",
    "If multicollinearity exists:\n",
    "\n",
    "Coefficients become unstable.\n",
    "\n",
    "The model may not converge properly.\n",
    "\n",
    "Interpretations become unreliable."
   ],
   "id": "9b036645619bb4ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>The Fix — Drop One Column</h3>\n",
    "Instead of using 3 columns, we drop one:\n"
   ],
   "id": "983e7d40473229c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Color | Color_Red | Color_Blue | Note                    |\n",
    "|-------|-----------|------------|-------------------------|\n",
    "| Red   | 1         | 0          |                         |\n",
    "| Blue  | 0         | 1          |                         |\n",
    "| Green | 0         | 0          | ← implicitly \"Green\"    |\n"
   ],
   "id": "8116a646f52f2c49"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now:\n",
    "<li>There’s no redundancy.</li>\n",
    "<li>The model still knows all categories (the dropped one is the reference).</li>\n",
    "<li>TThis avoids the dummy variable trap.</li>"
   ],
   "id": "1058d25db0931237"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Bottom Line</h3>\n",
    "<li>Dummy Variable Trap = Multicollinearity from including all one-hot encoded columns.</li>\n",
    "<li>Fix = Drop one category per variable (usually done automatically with drop='first' in OneHotEncoder).</li>"
   ],
   "id": "b4d7f258fd3ef001"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Linear regression requires this assumption: Independent variables must not be linearly dependent.",
   "id": "9590f6ceef697365"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Why It’s Also a Problem in Logistic Regression</h3>\n",
    "Even though logistic regression uses a different loss function (log-loss, not squared error), it also:\n",
    "<li>Computes gradients based on feature weights</li>\n",
    "<li>Solves for parameters using iterative optimization (like gradient descent or Newton-Raphson)</li>\n",
    "<hr>\n",
    "If there's perfect multicollinearity:\n",
    "<li>Gradient updates can’t converge reliably</li>\n",
    "<li>The optimization becomes numerically unstable</li>\n",
    "<li>Coefficients may flip wildly between epochs or become meaningless</li>"
   ],
   "id": "725382a3b16a75de"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Core of the Problem</h3>",
   "id": "7764d4e773eb6d2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<li>Including all one-hot columns makes one of them redundant (it contains no new information).</li>\n",
    "<li>This leads to mathematical instability in linear algebra computations inside the model.</li>\n",
    "<li>Linear and logistic regression require independent features — when you give them fully one-hot encoded data without dropping one column, you violate this rule.</li>"
   ],
   "id": "7b05993ed72a1310"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Safe Fix:</h3>",
   "id": "cc82221e748cccfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<p>Drop one category per categorical variable (using drop='first' or drop='if_binary' in OneHotEncoder).</p>\n",
    "\n",
    "This doesn’t lose information — it just:\n",
    "<li>Removes redundancy</li>\n",
    "<li>Keeps math stable</li>\n",
    "<li>Makes models more interpretable</li>"
   ],
   "id": "a08802831b606689"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
