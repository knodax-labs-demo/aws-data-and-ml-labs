{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h2>Exercise: Apply Tokenization on Text Data With NLTK and Store Processed Tokens in S3</h2>",
   "id": "41798d5f427bfef6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Set up dataset</h3>",
   "id": "a0c93f80c1dce930"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-27T23:59:07.708443Z",
     "start_time": "2025-07-27T23:59:06.252394Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Sample data (5 short reviews)\n",
    "#You can store this in a CSV, or just build it in code:\n",
    "data = {\n",
    "    \"review_id\": [1, 2, 3, 4, 5],\n",
    "    \"review_text\": [\n",
    "        \"I absolutely loved this product! It works great and the quality is fantastic.\",\n",
    "        \"Not bad, but shipping was slow... I might try a different seller next time.\",\n",
    "        \"Terrible experience. The item broke in two days and support was unhelpful.\",\n",
    "        \"Decent value for the price. Could be better packaged.\",\n",
    "        \"Amazing! Fast delivery and excellent customer service. Highly recommend.\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                                        review_text\n",
      "0          1  I absolutely loved this product! It works grea...\n",
      "1          2  Not bad, but shipping was slow... I might try ...\n",
      "2          3  Terrible experience. The item broke in two day...\n",
      "3          4  Decent value for the price. Could be better pa...\n",
      "4          5  Amazing! Fast delivery and excellent customer ...\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Download NLTK resources & set up helpers</h3>",
   "id": "15bed249180c6227"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T23:59:25.444904Z",
     "start_time": "2025-07-27T23:59:20.857744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ],
   "id": "153381f7b0715583",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sksingh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sksingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Tokenize + clean </h3>\n",
    "We'll tokenize the text using word_tokenize, convert tokens to lowercase, keep only alphabetic tokens using str.isalpha(), and remove English stop words."
   ],
   "id": "dc047494086f26df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:00:10.023236Z",
     "start_time": "2025-07-28T00:00:09.580106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def tokenize_and_clean(text: str):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t.lower() for t in tokens]               # lowercase\n",
    "    tokens = [t for t in tokens if t.isalpha()]        # remove punctuation/numbers\n",
    "    tokens = [t for t in tokens if t not in stop_words]# remove stop words\n",
    "    return tokens\n",
    "\n",
    "df[\"tokens\"] = df[\"review_text\"].apply(tokenize_and_clean)\n",
    "print(df[[\"review_id\", \"tokens\"]])\n"
   ],
   "id": "859e2f2db4af5b7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                                             tokens\n",
      "0          1  [absolutely, loved, product, works, great, qua...\n",
      "1          2  [bad, shipping, slow, might, try, different, s...\n",
      "2          3  [terrible, experience, item, broke, two, days,...\n",
      "3          4    [decent, value, price, could, better, packaged]\n",
      "4          5  [amazing, fast, delivery, excellent, customer,...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3># (Optional) Create an exploded (1 token per row) table</h3>",
   "id": "b13c469675143194"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:00:25.869331Z",
     "start_time": "2025-07-28T00:00:25.799321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens_exploded = df[[\"review_id\", \"tokens\"]].explode(\"tokens\") \\\n",
    "    .rename(columns={\"tokens\": \"token\"})\n",
    "print(tokens_exploded.head())\n"
   ],
   "id": "3563f9ab3ecc53a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id       token\n",
      "0          1  absolutely\n",
      "0          1       loved\n",
      "0          1     product\n",
      "0          1       works\n",
      "0          1       great\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Save to CSV locally</h3>",
   "id": "73c3833217fdb9ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:00:36.820584Z",
     "start_time": "2025-07-28T00:00:36.780837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df.to_csv(\"tokenized_reviews_nested.csv\", index=False)       # tokens as Python lists (stringified)\n",
    "tokens_exploded.to_csv(\"tokenized_reviews_exploded.csv\", index=False)  # one token per row\n"
   ],
   "id": "6b21ef317349c034",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Upload to S3 with boto3</h3>",
   "id": "71c2c7f3671ee74e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:01:58.266723Z",
     "start_time": "2025-07-28T00:01:57.882629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "\n",
    "region = \"us-east-1\"                    # change if needed\n",
    "bucket = \"knodax-feature-engineering\"             # <- change me\n",
    "prefix = \"nlp/\"                         # optional folder in the bucket\n",
    "\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "# Upload files\n",
    "s3.upload_file(\"tokenized_reviews_nested.csv\", bucket, f\"{prefix}tokenized_reviews_nested.csv\")\n",
    "s3.upload_file(\"tokenized_reviews_exploded.csv\", bucket, f\"{prefix}tokenized_reviews_exploded.csv\")\n",
    "\n",
    "print(\"Files uploaded to S3!\")\n"
   ],
   "id": "ae5b8f3127633754",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files uploaded to S3!\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>What was done</h3>\n",
    "<li>Tokenized and cleaned raw text using NLTK.</li>\n",
    "<li>Saved the processed tokens in both nested and exploded formats.</li>\n",
    "<li>Persisted the processed artifacts to Amazon S3 for future workflows (ETL, SageMaker, Glue, Athena, etc.).</li>"
   ],
   "id": "864c7b526a52b012"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For most machine learning (ML) workloads, the exploded format is typically preferred — especially when you are working on token-level tasks like:\n",
    "<li>Building vocabulary</li>\n",
    "<li>Training word embeddings (e.g., Word2Vec, FastText)</li>\n",
    "<li>Preparing inputs for sequence models (LSTM, Transformer)</li>\n",
    "<li>Token frequency analysis or TF-IDF</li>\n",
    "<li>Feeding tokenized inputs to NLP pipelines (e.g., Hugging Face models)</li>"
   ],
   "id": "93a4f34b54bfb32c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3> Why Exploded Format Is Preferred:</h3>\n",
    "\n"
   ],
   "id": "dd3a03b722d3fe31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Format       | Structure                                            | Pros                                                       | Cons                                         |\n",
    "| ------------ | ---------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------- |\n",
    "| **Exploded** | 1 row per token (`review_id`, `token`)               | Easy to aggregate, count, map vocab IDs, filter, vectorize | File is longer, not nested                   |\n",
    "| **Nested**   | 1 row per doc with list of tokens (`tokens = [...]`) | Better for direct reuse of full token lists per document   | Requires parsing; harder for per-token stats |\n"
   ],
   "id": "882181ee6186a2f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Example Use Case</h3>",
   "id": "27768d7f3b1c74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "| Use Case                       | Preferred Format                              |\n",
    "| ------------------------------ | --------------------------------------------- |\n",
    "| Word frequency/counts          | **Exploded**                                  |\n",
    "| TF-IDF / CountVectorizer       | **Exploded** or nested (depends on tool)      |\n",
    "| Sequence modeling (e.g., LSTM) | **Nested** (with padding/token index mapping) |\n",
    "| Word embeddings                | **Exploded**                                  |\n",
    "| Custom NLP pretraining         | **Exploded**                                  |\n"
   ],
   "id": "d4dc955cf5503621"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For data analysis and vocab generation, use the exploded format. For feeding into ML models, especially if batching sentences/documents, nested token lists may be useful — but often converted to tensors or padded arrays later on. So in practice, you often start with exploded format, then convert as needed.",
   "id": "bee4e554e909a5fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
