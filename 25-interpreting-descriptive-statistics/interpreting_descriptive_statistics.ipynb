{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Load the Sample Dataset</h3>",
   "id": "31037f503d51fdc5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-28T22:22:35.642168Z",
     "start_time": "2025-07-28T22:22:34.413412Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('s3://knodax-ml-datasets/demographic_scores.csv')\n",
    "df.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   id  age  income  score category\n",
       "0   1   25   48000     78        A\n",
       "1   2   30   52000     85        A\n",
       "2   3   22   35000     65        B\n",
       "3   4   28   49000     80        A\n",
       "4   5   35   61000     88        A"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>score</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>48000</td>\n",
       "      <td>78</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>52000</td>\n",
       "      <td>85</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>35000</td>\n",
       "      <td>65</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>49000</td>\n",
       "      <td>80</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>61000</td>\n",
       "      <td>88</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Compute Descriptive Statistics</h3>\n",
    "Use pandas' describe() method to generate statistics such as mean, median, standard deviation, minimum, and maximum for numerical features."
   ],
   "id": "f591cbe435138169"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T22:22:46.553346Z",
     "start_time": "2025-07-28T22:22:46.530045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "desc_stats = df.describe()\n",
    "print(desc_stats)\n"
   ],
   "id": "a62b5673e08716e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id        age      income      score\n",
      "count  10.00000  10.000000     10.0000  10.000000\n",
      "mean    5.50000  28.100000  48700.0000  77.400000\n",
      "std     3.02765   6.297266  11450.8612  10.276186\n",
      "min     1.00000  21.000000  34000.0000  60.000000\n",
      "25%     3.25000  23.250000  40250.0000  70.500000\n",
      "50%     5.50000  26.500000  48500.0000  79.000000\n",
      "75%     7.75000  32.250000  57250.0000  85.750000\n",
      "max    10.00000  40.000000  68000.0000  90.000000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Compute Correlation Matrix</h3>\n",
    "Use the corr() method to identify relationships between numerical columns. This helps you evaluate multicollinearity and feature interactions."
   ],
   "id": "4b236dd6879e1dec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T22:22:56.479147Z",
     "start_time": "2025-07-28T22:22:56.469009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "correlation_matrix = df.select_dtypes(include='number').corr() # df.corr() in pandas tries to compute Pearson correlation, which only works with numeric columns.\n",
    "print(correlation_matrix)\n"
   ],
   "id": "e41509985f275284",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id       age    income     score\n",
      "id      1.000000  0.067019  0.014422 -0.107137\n",
      "age     0.067019  1.000000  0.981999  0.917915\n",
      "income  0.014422  0.981999  1.000000  0.960492\n",
      "score  -0.107137  0.917915  0.960492  1.000000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4>Key Interpretations:</h4>\n",
    "id has low correlation with all other variables:<br>\n",
    "<li>id vs age: 0.067 → very weak positive correlation (basically no meaningful relation)</li>\n",
    "<li>id vs income: 0.014 → negligible correlation</li>\n",
    "<li>id vs score: -0.107 → weak negative correlation</li>\n",
    "<li>Suggests id is just an index and not related to features; you can drop it for analysis.</li><br>\n",
    "\n",
    "age vs income: 0.982\n",
    "<li>Strong positive correlation — as age increases, income tends to increase.</li>\n",
    "<li>Likely because more experienced people earn more.</li><br>\n",
    "\n",
    "age vs score: 0.918\n",
    "<li>Strong positive correlation — older people tend to have higher scores.</li><br>\n",
    "\n",
    "income vs score: 0.960\n",
    "<li>Very strong positive correlation — people with higher income tend to have higher scores.</li><br>\n",
    "\n",
    "Summary: <br>\n",
    "<li>Ignore id for analysis.</li>\n",
    "<li>age, income, and score are strongly correlated with each other.</li>\n",
    "<li>These relationships suggest redundancy — possibly relevant for PCA or feature selection.</li>"
   ],
   "id": "618a971e623b7f22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In machine learning, highly correlated features (also called multicollinearity) can indeed be a problem, depending on the model you're using.\n",
    "\n",
    "Here's why and what to do:\n",
    "\n",
    "<h4>Why Highly Correlated Features Can Be Problematic</h4>\n",
    "Redundancy: Highly correlated features carry similar information. This doesn't help the model learn better — it just adds noise or redundancy.\n",
    "\n",
    "Multicollinearity (especially in linear models like Linear Regression, Logistic Regression): It makes it hard to determine the individual effect of each feature. Model coefficients become unstable and hard to interpret.\n",
    "\n",
    "Overfitting Risk: Redundant features can make a model more complex than necessary, increasing the risk of overfitting."
   ],
   "id": "35ef518ceb2aeca3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4>What You Can Do (Common Approaches)</h4>\n",
    "Remove One of the Correlated Features: If age, income, and score are > 0.9 correlated, you might drop one or two of them. Keep the one that makes the most domain sense or is most interpretable.<br>\n",
    "\n",
    "Dimensionality Reduction (like PCA): PCA transforms correlated variables into uncorrelated principal components. You lose interpretability, but gain compact, cleaner data for modeling. <br>\n",
    "\n",
    "Use Tree-Based Models (Random Forest, XGBoost): These models handle multicollinearity better than linear models. But still, fewer redundant features can improve speed and simplicity. <br>\n",
    "\n",
    "In summary, in many ML workflows, highly correlated features are removed or combined to reduce redundancy. This helps improve model interpretability, generalization, and sometimes even accuracy."
   ],
   "id": "c4a9b8f2f8a2b4d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>Perform Hypothesis Testing with SciPy</h3>\n",
    "Use SciPy’s statistical test functions to assess whether observed differences between groups are statistically significant."
   ],
   "id": "c1d996eea027aac2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T22:24:07.332583Z",
     "start_time": "2025-07-28T22:24:05.538726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "group_a = df[df['category'] == 'A']['income']\n",
    "group_b = df[df['category'] == 'B']['income']\n",
    "\n",
    "t_stat, p_value = ttest_ind(group_a, group_b)\n",
    "print(f\"T-Statistic: {t_stat}, P-Value: {p_value}\")\n"
   ],
   "id": "ccdfb08bee713b38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Statistic: 4.411064394627796, P-Value: 0.002253201469264483\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The difference between the two groups is statistically significant — it’s very unlikely that the difference you observed is due to random chance.",
   "id": "b35a9d4082f54df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h4>1. T-Statistic</h4>\n",
    "The t-statistic measures how much the means of two groups differ in terms of standard error. A higher absolute value of the t-statistic means a bigger difference between groups relative to variation.\n",
    "\n",
    "In your case:\n",
    "T = 4.411 → suggests a strong difference between group means.\n",
    "\n",
    "<h4>2. P-Value</h4>\n",
    "The p-value tells you the probability of observing this result (or more extreme) if the null hypothesis were true. The null hypothesis typically assumes no difference between the groups. Your p-value is 0.00225, which is very small.\n",
    "\n",
    "<h4>3. Interpretation</h4>\n",
    "If you're using a common threshold (significance level) of: α = 0.05 → You reject the null hypothesis if p < 0.05 In this case: p = 0.00225 < 0.05, so you reject the null hypothesis."
   ],
   "id": "62fa10103c3f48a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
